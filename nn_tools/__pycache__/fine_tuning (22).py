# -*- coding: utf-8 -*-
"""fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IPm7ZGYWkhf5xoWLSI8_o5V0KUobstgC
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/MA2/Master Thesis/project')

import numpy as np
import copy

import torch
from datasets import load_dataset
from torch.utils.data import DataLoader

from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification, BertModel
from sklearn.metrics import accuracy_score

from nn_tools.nn_compression import Model
import evaluate as t5_evaluate

'''
def compute_metrics(eval_pred):
  logits, labels = eval_pred
  predictions = np.argmax(logits, axis=-1)
  accuracy = accuracy_score(labels, predictions)
  return {"accuracy": accuracy}
'''

def get_tokenize_function(dataset_name, model_name, tokenizer):
    """Returns the appropriate tokenization function for the dataset and model type"""
    # First define the column names for each dataset
    dataset_columns = {
        "cola": ("sentence", None),
        "sst2": ("sentence", None),
        "mrpc": ("sentence1", "sentence2"),
        "qqp": ("question1", "question2"),
        "mnli": ("premise", "hypothesis"),
        "qnli": ("question", "sentence"),
        "rte": ("sentence1", "sentence2"),
        "wnli": ("sentence1", "sentence2")
    }

    if dataset_name not in dataset_columns:
        raise ValueError(f"Unknown dataset name: {dataset_name}")

    col1, col2 = dataset_columns[dataset_name]

    if model_name == "t5":
        def t5_tokenize_function(examples):
            inputs      = [f"cola sentence: {s}" for s in examples["sentence"]]
            label_texts = ["acceptable" if l==1 else "unacceptable" for l in examples["label"]]

            model_inputs    = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
            labels_tokenized= tokenizer(label_texts, max_length=10, truncation=True, padding="max_length")
            model_inputs["labels"] = labels_tokenized["input_ids"]
            return model_inputs

        return t5_tokenize_function
    else:
        def bert_tokenize_function(examples):
            if col2:
                return tokenizer(
                    examples[col1],
                    examples[col2],
                    padding="max_length",
                    truncation=True,
                    max_length=128,
                    return_tensors="pt"
                )
            else:
                return tokenizer(
                    examples[col1],
                    padding="max_length",
                    truncation=True,
                    max_length=128,
                    return_tensors="pt"
                )

        return bert_tokenize_function

def fine_tune(raw_Model, dataset_name, train_fraction=1, test_fraction=1, n_epochs=3, batch_size=16):
    '''
    Fine tune a model on a dataset from the GLUE benchmark

    Args:
        raw_Model (object): Model we want to fine-tune (should have .model attribute)
        tokenizer: Tokenizer for the model
        dataset_name (str): Name of GLUE dataset ('cola', 'sst2', 'mrpc', 'stsb', 'qqp', 'mnli', 'qnli', 'rte', 'wnli')
        train_fraction (float): Fraction of training data to use (0-1)
        test_fraction (float): Fraction of validation data to use for testing (0-1)
        n_epochs (int): Number of training epochs
        batch_size (int): Batch size for training/evaluation

    Returns:
        object: Fine-tuned model
    '''

    model = raw_Model.model
    tokenizer = raw_Model.tokenizer
    dataset = load_dataset('glue', dataset_name)
    model_name = raw_Model.name

    # For all GLUE tasks, we use 'validation' split as our test set
    train_dataset = dataset['train'].select(range(int(len(dataset['train']) * train_fraction)))
    test_dataset = dataset['validation'].select(range(int(len(dataset['validation']) * test_fraction)))

    tokenize_function = get_tokenize_function(dataset_name, model_name, tokenizer)

    # Tokenize datasets
    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)

    def compute_metrics(eval_pred):
      if model_name == "t5":
          metric = t5_evaluate.load("glue", dataset_name)
          label_list = metric.info.label_list

          raw_preds, labels = eval_preds
          # unpack if HF returns (logits, past_key_values)
          logits = raw_preds[0] if isinstance(raw_preds, tuple) else raw_preds

          # for each position take the highest‐scoring token
          token_ids = np.argmax(logits, axis=-1)        # shape (batch, seq_len)
          decoded = tokenizer.batch_decode(token_ids, skip_special_tokens=True)

          # map each decoded string to its index in label_list
          preds = [label_list.index(txt.strip()) for txt in decoded]

          return metric.compute(predictions=preds, references=labels)
      else:
          # Original BERT-style metric computation
          logits, labels = eval_pred
          preds = np.argmax(logits, axis=-1)

      return {"accuracy": (preds == labels).astype(np.float32).mean().item()}

    training_args = TrainingArguments(
      output_dir='./results',          # Directory to save model checkpoints
      #evaluation_strategy="epoch",
      per_device_train_batch_size=batch_size,
      per_device_eval_batch_size=batch_size,
      num_train_epochs=n_epochs,
      #save_strategy="epoch",          # Save model at the end of each epoch
      save_total_limit=2,             # Keep only the last 2 checkpoints
      report_to="none",               # Disable wandb/tensorboard)
      )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_dataset,
        eval_dataset=tokenized_test_dataset,
        compute_metrics=compute_metrics,
    )

    trainer.train()

    fine_tuned_Model = Model(name=raw_Model.name, task=raw_Model.task,
                            weight_path=None, set_model=True, model=model)
    fine_tuned_Model.tokenizer = raw_Model.tokenizer

    return fine_tuned_Model

def evaluate(raw_Model, dataset_name, split="validation", batch_size=16, n_epochs=3):
    model      = raw_Model.model
    tokenizer  = raw_Model.tokenizer
    model_name = raw_Model.name

    # 1) load dataset and extract label‐names
    ds = load_dataset("glue", dataset_name)
    label_list = ds["train"].features["label"].names     # ← here

    # 2) prepare tokenized eval set
    tokenize_fn = get_tokenize_function(dataset_name, model_name, tokenizer)
    tokenized_test = ds[split].map(tokenize_fn, batched=True)

    def get_compute_metrics(tokenizer, dataset_name, model_name):

      if model_name == 't5' :
        metric = t5_evaluate.load("glue", dataset_name)

        def compute_metrics(eval_preds):
            raw_preds, labels = eval_preds

            # Unpack logits if needed
            if isinstance(raw_preds, tuple):
                logits = raw_preds[0]
            else:
                logits = raw_preds

            # Get predicted tokens and decode
            token_preds = np.argmax(logits, axis=-1)
            decoded_preds = tokenizer.batch_decode(token_preds, skip_special_tokens=True)
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            # Dataset-specific label mapping
            if dataset_name == "cola":
                # CoLA: "acceptable" -> 1, others -> 0
                bin_preds = [1 if p.strip() == "acceptable" else 0 for p in decoded_preds]
                bin_labels = [1 if l.strip() == "acceptable" else 0 for l in decoded_labels]

            elif dataset_name == "mrpc":
                # MRPC: "equivalent" -> 1, "not_equivalent" -> 0
                bin_preds = [1 if p.strip() == "equivalent" else 0 for p in decoded_preds]
                bin_labels = [1 if l.strip() == "equivalent" else 0 for l in decoded_labels]

            elif dataset_name in ["rte", "qnli"]:
                # RTE and QNLI: "entailment" -> 1, "not_entailment" -> 0
                bin_preds = [1 if p.strip() == "entailment" else 0 for p in decoded_preds]
                bin_labels = [1 if l.strip() == "entailment" else 0 for l in decoded_labels]

            else:
                raise ValueError(f"Unsupported dataset: {dataset_name}")

            return metric.compute(predictions=bin_preds, references=bin_labels)

      else :

        def compute_metrics(eval_preds):
          logits, labels = eval_pred
          preds = np.argmax(logits, axis=-1)

          return {"accuracy": (preds == labels).astype(np.float32).mean().item()}


      return compute_metrics

    compute_metrics = get_compute_metrics(tokenizer=tokenizer, dataset_name=dataset_name, model_name=model_name)

    # 5) set up Trainer
    training_args = TrainingArguments(
        output_dir="./results",
        per_device_eval_batch_size=batch_size,
        per_device_train_batch_size=batch_size,
        num_train_epochs=n_epochs,
        report_to="none",
    )
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=None,
        eval_dataset=tokenized_test,
        compute_metrics=compute_metrics,
    )

    # 6) run evaluation
    results = trainer.evaluate()
    print(f"eval accuracy: {results['eval_accuracy']}")
    return results["eval_accuracy"]

def get_tokenized_dataset(Model, dataset_name, split, train_fraction=1, test_fraction=1):
    '''
    Returnes a tokenized dataset from the GLUE benchmark. The tokenization is done accordingly to the model passed as an argument

    Args:
        raw_Model (object): Model we want to fine-tune (should have .model attribute)
        dataset_name (str): Name of GLUE dataset ('cola', 'sst2', 'mrpc', 'stsb', 'qqp', 'mnli', 'qnli', 'rte', 'wnli')
        split (str) : e.g. 'train', 'validation', 'test'
        train_fraction (float): Fraction of training data to use (0-1)
        test_fraction (float): Fraction of validation data to use for testing (0-1)

    Returns:
        object: tokenized dataset
    '''

    model = Model.model
    tokenizer = Model.tokenizer
    dataset = load_dataset('glue', dataset_name)
    dataset = dataset[split].select(range(int(len(dataset[split]) * test_fraction))) #selects dataset split

    if dataset_name == "cola":
        def tokenize_function(examples):
            return tokenizer(examples['sentence'], padding="max_length", truncation=True)

    elif dataset_name == "sst2":
        def tokenize_function(examples):
            return tokenizer(examples['sentence'], padding="max_length", truncation=True)

    elif dataset_name == "mrpc":
        def tokenize_function(examples):
            return tokenizer(examples['sentence1'], examples['sentence2'],
                           padding="max_length", truncation=True)

    elif dataset_name == "qqp":
        def tokenize_function(examples):
            return tokenizer(examples['question1'], examples['question2'],
                           padding="max_length", truncation=True)

    elif dataset_name == "mnli":
        def tokenize_function(examples):
            return tokenizer(examples['premise'], examples['hypothesis'],
                           padding="max_length", truncation=True)

    elif dataset_name == "qnli":
        def tokenize_function(examples):
            return tokenizer(examples['question'], examples['sentence'],
                           padding="max_length", truncation=True)

    elif dataset_name == "rte":
        def tokenize_function(examples):
            return tokenizer(examples['sentence1'], examples['sentence2'],
                           padding="max_length", truncation=True)

    elif dataset_name == "wnli":
        def tokenize_function(examples):
            return tokenizer(examples['sentence1'], examples['sentence2'],
                           padding="max_length", truncation=True)
    else:
        raise ValueError(f"Unknown dataset name: {dataset_name}")

    # Tokenize datasets
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

    return tokenized_dataset

def sample_tokenized_dataset(tokenized_dataset, batch_size) :
  dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)
  batch = next(iter(dataloader))
  return batch