# -*- coding: utf-8 -*-
"""nn_compression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qG8OFPjoVPLZxel6cePi9I7AQ0XMKKYC
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/MA2/Master Thesis/project')

import numpy as np
import matplotlib.pyplot as plt
import copy
import math
import time

import torch
import torch.nn as nn
from torch.nn import Sequential
import torch.nn.functional as F

from transformers import BertModel, BertForSequenceClassification, BertTokenizer

from nn_tools import algebraic as alg

class Model:

  def __init__(self, name, task, weight_path, **kwargs):
    '''
      Class that allows to load a model, and compute some operations on it (e.g. random forward pass on target layers)

      intput parameters :
        name (str) : e.g. "bert"
        task (str) : e.g. "pretrained", "classification"
        weight_path (str) : path through the weights of the model we want to load (None if we initialize a base model)
        kwargs : {"n_class" (int) : number of classes if classification model
                }

      class attributes :
        self.device : device on which data and model are stored
        self.model (pytorch) : pytorch model
    '''

    if name not in ["bert", "gpt2"]:
        raise ValueError(f"Invalid name: {name}. Expected one of: 'bert', 'gpt2'.")
    if task not in ["pretrained", "classification"]:
        raise ValueError(f"Invalid task: {task}. Expected one of: 'pretrained', 'classification'.")

    self.name = name
    self.task = task
    self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Handle kwargs
    set_model = kwargs.get("set_model", False)

    if set_model:
      # Set the model manually
      if "model" not in kwargs:
          raise ValueError("'model' must be provided when 'set_model' is True.")
      self.model = kwargs["model"].to(self.device)
    else:
      #BERT
      if self.name == "bert" :
        if self.task == "pretrained":
          if weight_path is not None :
            #self.model.load_state_dict(torch.load(weight_path))
            self.model = torch.load(weight_path, weights_only=False)
          else :
            self.model = BertModel.from_pretrained('bert-base-uncased').to(self.device)

        elif self.task == "classification":
          if weight_path is not None :
            #self.model.load_state_dict(torch.load(weight_path))
            self.model = torch.load(weight_path, weights_only=False)
          else :
            self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=kwargs["n_class"]).to(self.device)

        else :
          raise ValueError("Invalid task : expected one of: 'pretrained', 'classification'")

        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

      #GPT-2 (TO DO -> model + tokenizer)
      elif self.name == "gpt2" :
        print('TO DO')

      else :
        raise ValueError("Invalid name : expected one of: 'bert', 'gpt2'")


    self.svd_dict = {}


  def layer_name(self, block, layer_idx):
    '''
      Args :
        block (str) : e.g. "key" ~ architectural block type
        layer_idx (int) : e.g. 5 ~ layer index

      Returns :
        (str) : complete layer name
    '''

    #BERT
    if self.name == "bert" :
      if self.task == "pretrained" :
        if block == "key":
          return "encoder.layer." + str(layer_idx) + ".attention.self.key"

        elif block == "query":
            return "encoder.layer." + str(layer_idx) + ".attention.self.query"

        elif block == "value":
            return "encoder.layer." + str(layer_idx) + ".attention.self.value"

        elif block == "inter":
            return "encoder.layer." + str(layer_idx) + ".intermediate.dense"

        elif block == "output":
            return "encoder.layer." + str(layer_idx) + ".output.dense"

        else:
            raise ValueError("Invalid block : expected one of: 'key', 'query', 'value', 'inter', 'output'")

      else :
        if block == "key":
          return "bert.encoder.layer." + str(layer_idx) + ".attention.self.key"

        elif block == "query":
            return "bert.encoder.layer." + str(layer_idx) + ".attention.self.query"

        elif block == "value":
            return "bert.encoder.layer." + str(layer_idx) + ".attention.self.value"

        elif block == "inter":
            return "bert.encoder.layer." + str(layer_idx) + ".intermediate.dense"

        elif block == "output":
            return "bert.encoder.layer." + str(layer_idx) + ".output.dense"

        else:
            raise ValueError("Invalid block : expected one of: 'key', 'query', 'value', 'inter', 'output'")

    #GPT-2 (TO DO)



  def weight_matrix(self, layer_name):
    '''
      Returns the weight matrix corresponding to a given block and layer

      Args :
        layer_name (str) : e.g.

      Returns
        (pytorch tensor) : weight matrix correspong to given block at given layer
    '''

    if self.name == "bert" :
      return self.model.state_dict()[layer_name + str(".weight")]

    else :
       raise ValueError("Invalid block : expected one of: 'bert'")


  def layer_forward_pass(self, input, layer_name) :
    '''
      Computes forward pass on a single target layer

      Args :
        input (pytorch tensor [batch, seq_len, hidden_size]) : hidden_size usually equals 768
        layer_name (str) : e.g.

      Returns :
        (pytorch tensor) : [batch, seq_len, output_size] weight matrix correspong to given block at given layer
    '''

    layer_path = layer_name.split('.')
    layer = self.model

    for part in layer_path:
        if part.isdigit():
            part = int(part)
            layer = layer[part]
        else:
            layer = getattr(layer, part)

    with torch.no_grad():
        output = layer(input)

    return output


  def forward_pass(self, input, skip_embedding_layer=True) :
    '''
      Computes forward pass on the entire model (except word embedding layer if skip_embedding_layer==True)

      Args :
        input (pytorch tensor) : [batch, seq_len, hidden_size] hidden_size usually equals 768
        skip_embedding_layer (bool) : if we skip the word embedding layer during forward pass

      Returns :
        (pytorch tensor) : output of the model
    '''

    with torch.no_grad():
      if skip_embedding_layer :

          #BERT
          if self.name == "bert" :
            if self.task == "pretrained" :
              return self.model.encoder(input).last_hidden_state

            elif self.task == "classification" :
              return self.model.bert.encoder(input).last_hidden_state

          #GPT-2 (TO DO)


  def model_svd(self, compressed_blocks) :
    """
      Compute full SVD factorization of the model's weight matrices

      Args:
          compressed_blocks (str array) : type of blocks we want to compute the SVD for (e.g. ['output', 'key', 'query', 'value', 'intermediate'])
      """

    if len(self.svd_dict) == 0: #if the dictionnary is empty
      if self.name == "bert" :
        for name, param in self.model.state_dict().items():
          if 'weight' in name and any(substring in name for substring in compressed_blocks):
              if len(param.shape) == 2:  # only compress weight matrices
                layer_name = str(name)[:-7]
                W = self.weight_matrix(layer_name).cpu().detach().numpy()
                self.svd_dict[layer_name] = {}
                U, S, V = np.linalg.svd(W, full_matrices=False)
                self.svd_dict[layer_name]["U"] = U
                self.svd_dict[layer_name]["S"] = S
                self.svd_dict[layer_name]["V"] = V


  def replace_layer_with_low_rank(self, layer_name, rank_ratio):
      """
      Replace a given layer in a copied model with the product of two low-rank layers

      Args:
          model (torch.nn.Module) : The pre-trained model to copy and modify
          layer_name (str) : name of the layer we want to compress
          rank_ratio (float): The rank ratio for the low-rank approximation

      Returns:
          torch.nn.Module : A copy of the modified model
      """

      # Create a deep copy of the model
      modified_model = copy.deepcopy(self.model)

      # Access the target layer
      module_parts = layer_name.split('.')
      parent_module = modified_model

      for part in module_parts[:-1]:
          parent_module = getattr(parent_module, part)

      target_layer_name = module_parts[-1]
      original_layer = getattr(parent_module, target_layer_name)

      # Get the weight and bias of the original layer
      W = original_layer.weight.data  # (out_features, in_features)
      bias = original_layer.bias.data  # (out_features)

      # Perform SVD
      full_r = min(W.shape)
      rank = int(full_r * rank_ratio)

      if layer_name in self.svd_dict :
        U = torch.tensor(self.svd_dict[layer_name]["U"]).to(self.device)
        S = torch.tensor(self.svd_dict[layer_name]["S"] ).to(self.device)
        Vh = torch.tensor(self.svd_dict[layer_name]["V"]).to(self.device)

      else :
        print("compute SVD from scratch")
        U, S, Vh = torch.linalg.svd(W, full_matrices=False)

      U_r = U[:, :rank]       # (out_features, rank)
      S_r = S[:rank]          # (rank,)
      Vh_r = Vh[:rank, :]     # (rank, in_features)

      # Create low-rank layers
      fc1_A = nn.Linear(Vh_r.shape[1], rank, bias=False)  # First layer (A)
      fc1_B = nn.Linear(rank, U_r.shape[0], bias=True)    # Second layer (B)

      # Set weights for the low-rank layers
      fc1_A.weight.data = Vh_r  # V^T
      fc1_B.weight.data = (U_r * S_r)  # U * S
      fc1_B.bias.data = bias  # Transfer the original bias

      # Combine into a sequential layer and replace the layer in the parent module
      new_layer = nn.Sequential(fc1_A, fc1_B)
      setattr(parent_module, target_layer_name, new_layer)
      self.model = modified_model

def layer_random_output_error(original_model, compressed_model, block, layer_idx, batch, seq_len, hidden_size, mu, sigma) :
  '''
    Computes the output of a base model and compare it to the output of its compressed version
    The input is random (entries are i.i.d from N(mu, sigma)) and passes through a single layer (block, layer_idx)

    Args :
      original_model (torch.nn.Module) : full version of the model
      compressed_model (torch.nn.Module) : compressed version of the model
      block (str) : architectural block type e.g. "key"
      layer_idx (int) : layer index
      batch (int) : batch size of the random input
      seq_len (int) : sequence length of the random input
      hidden_size (int) : hidden size of the random input
      mu, sigma (int) : parameters of the normal distribution

    Returns :
      (float) : normalized frobenius norm between the 2 outputs
  '''

  if original_model.device == compressed_model.device :
    input = np.random.normal(mu, sigma, (batch, seq_len, hidden_size))
    input = torch.tensor(input, dtype=torch.float32).to(original_model.device)

    Y = original_model.layer_forward_pass(input, block, layer_idx).detach().cpu().numpy()
    Y = Y.reshape((batch, seq_len*Y.shape[2]))
    Y_ = compressed_model.layer_forward_pass(input, block, layer_idx).detach().cpu().numpy()
    Y_ = Y_.reshape((batch, seq_len*Y_.shape[2]))

    return np.linalg.norm(Y - Y_, 'fro')/(Y.shape[0]*Y.shape[1])

def random_output_error(original_model, compressed_model, batch, seq_len, hidden_size, mu, sigma) :
  '''
    Computes the output of a base model and compare it to the output of its compressed version
    The input is random (entries are i.i.d from N(mu, sigma)) and passes through the entire model

    Args :
      original_model (torch.nn.Module) : full version of the model
      compressed_model (torch.nn.Module) : compressed version of the model
      block (str) : architectural block type e.g. "key"
      layer_idx (int) : layer index
      batch (int) : batch size of the random input
      seq_len (int) : sequence length of the random input
      hidden_size (int) : hidden size of the random input
      mu, sigma (int) : parameters of the normal distribution

    Returns :
      (float) : normalized frobenius norm between the 2 outputs
  '''

  if original_model.device == compressed_model.device :
    input = np.random.normal(mu, sigma, (batch, seq_len, hidden_size))
    input = torch.tensor(input, dtype=torch.float32).to(original_model.device)

    Y = original_model.forward_pass(input).detach().cpu().numpy()
    Y = Y.reshape((batch, seq_len*Y.shape[2]))
    Y_ = compressed_model.forward_pass(input).detach().cpu().numpy()
    Y_ = Y_.reshape((batch, seq_len*Y_.shape[2]))

    return np.linalg.norm(Y - Y_, 'fro')/(Y.shape[0]*Y.shape[1])

def count_parameters(model):
  '''
    counts the number of learnable parameters in a model
  '''

  return sum(p.numel() for p in model.parameters() if p.requires_grad)

def knapsack_greedy_compression(Model, compressed_blocks=['output', 'key', 'query', 'value', 'intermediate'],
                               min_rank_ratios=np.linspace(0.1, 0.7, 7), max_rank_ratio=0.7,
                               rank_ratio_step=0.1, output_threshold=0.0015, batch=16, seq_len=10,
                               hidden_size=768, mu=0, sigma=1):

    max_rank_reached = False
    best_min_rank_ratio = 0
    best_resulting_param_ratio = 1
    best_Model = copy.deepcopy(Model)

    print("computing SVD of the model's layers... \n")
    Model.model_svd(compressed_blocks)

    # Precompute the weight dictionary
    weight_dict = {}
    for name, param in Model.model.state_dict().items():
        if 'weight' in name and any(substring in name for substring in compressed_blocks):
            if len(param.shape) == 2:  # only compress weight matrices

                layer_name = str(name)[:-7]
                sv_area = alg.compute_sv_area(sv_input=True, sv=Model.svd_dict[layer_name]["S"])
                n_param = param.shape[0] * param.shape[1]
                value = n_param * sv_area
                weight_dict[int(value)] = layer_name

    # Sort the dictionary by value in descending order
    sorted_dict = dict(sorted(weight_dict.items(), key=lambda item: item[0], reverse=True))
    sorted_weight_names = list(sorted_dict.values())

    print('start of the compression algorithm... \n')
    # Iterate over min_rank_ratios
    for min_rank_ratio in min_rank_ratios:
        print(f'min rank ratio : {min_rank_ratio}')

        current_rank_ratio = min_rank_ratio
        max_rank_reached = False
        lr_Model = copy.deepcopy(Model)

        for layer_name in sorted_weight_names:
            if max_rank_reached:
                continue

            temp_Model = copy.deepcopy(lr_Model)
            temp_Model.replace_layer_with_low_rank(layer_name, current_rank_ratio)
            output_error = random_output_error(lr_Model, temp_Model, batch, seq_len, hidden_size, mu, sigma)

            while output_error > output_threshold and not max_rank_reached:
                current_rank_ratio += rank_ratio_step
                if current_rank_ratio > max_rank_ratio:
                    max_rank_reached = True
                    break

                temp_Model = copy.deepcopy(lr_Model)
                temp_Model.replace_layer_with_low_rank(layer_name, current_rank_ratio)
                output_error = random_output_error(lr_Model, temp_Model, batch, seq_len, hidden_size, mu, sigma)

            if not max_rank_reached:
                lr_Model = copy.deepcopy(temp_Model)

        resulting_param_ratio = count_parameters(lr_Model.model) / count_parameters(Model.model)
        print(f'resulting_param_ratio : {resulting_param_ratio}')
        if resulting_param_ratio < best_resulting_param_ratio:
            best_resulting_param_ratio = resulting_param_ratio
            best_min_rank_ratio = min_rank_ratio
            best_Model = copy.deepcopy(lr_Model)

    print(f'best initial compression ratio : {best_min_rank_ratio}')
    print(f'best resulting relative size : {best_resulting_param_ratio}')

    return best_Model

def fool_svd_compression(Model, compressed_blocks=['output', 'key', 'query', 'value', 'intermediate'], rank_ratio=1) :

    print("computing SVD of the model's layers... \n")
    Model.model_svd(compressed_blocks)

    for name, param in Model.model.state_dict().items():
        if 'weight' in name and any(substring in name for substring in compressed_blocks):
            if len(param.shape) == 2:
                layer_name = str(name)[:-7]
                Model.replace_layer_with_low_rank(layer_name, rank_ratio)
    Model.svd_dict = {}
    return Model