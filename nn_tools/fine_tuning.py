# -*- coding: utf-8 -*-
"""fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IPm7ZGYWkhf5xoWLSI8_o5V0KUobstgC
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/MA2/Master Thesis/project')

import numpy as np
import copy

import torch
from datasets import load_dataset
from torch.utils.data import DataLoader

from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification, BertModel
from sklearn.metrics import accuracy_score
from transformers import DataCollatorWithPadding
from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score

from nn_tools.nn_compression import Model


def get_tokenize_function(dataset_name, model_name, tokenizer):
    """Returns the appropriate tokenization function for the dataset and model type"""
    # First define the column names for each dataset
    dataset_columns = {
        "cola": ("sentence", None),
        "sst2": ("sentence", None),
        "mrpc": ("sentence1", "sentence2"),
        "qqp": ("question1", "question2"),
        "mnli": ("premise", "hypothesis"),
        "qnli": ("question", "sentence"),
        "rte": ("sentence1", "sentence2"),
        "wnli": ("sentence1", "sentence2")
    }

    if dataset_name not in dataset_columns:
        raise ValueError(f"Unknown dataset name: {dataset_name}")

    col1, col2 = dataset_columns[dataset_name]

    def tokenize_function(examples):
        if col2:
            return tokenizer(
                examples[col1],
                examples[col2],
                padding="max_length",
                truncation=True,
                max_length=128,
                return_tensors="pt"
            )
        else:
            return tokenizer(
                examples[col1],
                padding="max_length",
                truncation=True,
                max_length=128,
                return_tensors="pt"
            )

    return tokenize_function

def fine_tune(raw_Model, dataset_name, train_fraction=1, test_fraction=1, n_epochs=3, batch_size=16):
    '''
    Fine tune a model on a dataset from the GLUE benchmark

    Args:
        raw_Model (object): Model we want to fine-tune (should have .model attribute)
        tokenizer: Tokenizer for the model
        dataset_name (str): Name of GLUE dataset ('cola', 'sst2', 'mrpc', 'stsb', 'qqp', 'mnli', 'qnli', 'rte', 'wnli')
        train_fraction (float): Fraction of training data to use (0-1)
        test_fraction (float): Fraction of validation data to use for testing (0-1)
        n_epochs (int): Number of training epochs
        batch_size (int): Batch size for training/evaluation

    Returns:
        object: Fine-tuned model
    '''

    model = raw_Model.model
    tokenizer = raw_Model.tokenizer
    dataset = load_dataset('glue', dataset_name)
    model_name = raw_Model.name

    #For all GLUE tasks, we use 'validation' split as our test set
    train_dataset = dataset['train'].select(range(int(len(dataset['train']) * train_fraction)))
    test_dataset = dataset['validation'].select(range(int(len(dataset['validation']) * test_fraction)))

    tokenize_function = get_tokenize_function(dataset_name, model_name, tokenizer)

    #Tokenize datasets
    tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
    tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)

    def compute_metrics(eval_pred):
      logits, labels = eval_pred
      predictions = np.argmax(logits, axis=-1)
      accuracy = accuracy_score(labels, predictions)
      return {"accuracy": accuracy}

    training_args = TrainingArguments(
      output_dir='./results',          
      #evaluation_strategy="epoch",
      per_device_train_batch_size=batch_size,
      per_device_eval_batch_size=batch_size,
      num_train_epochs=n_epochs,
      #save_strategy="epoch",         
      save_total_limit=2,             
      report_to="none",               
      )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train_dataset,
        eval_dataset=tokenized_test_dataset,
        compute_metrics=compute_metrics,
    )

    trainer.train()

    fine_tuned_Model = Model(name=raw_Model.name, task=raw_Model.task,
                            weight_path=None, set_model=True, model=model)
    fine_tuned_Model.tokenizer = raw_Model.tokenizer

    return fine_tuned_Model

def evaluate(raw_Model, dataset_name, split="validation", batch_size=16, n_epochs=3):
    model      = raw_Model.model
    tokenizer  = raw_Model.tokenizer
    model_name = raw_Model.name

    ds = load_dataset("glue", dataset_name)
    label_list = ds["train"].features["label"].names     # ‚Üê here

    tokenize_fn = get_tokenize_function(dataset_name, model_name, tokenizer)
    tokenized_test = ds[split].map(tokenize_fn, batched=True)

    def compute_metrics(eval_pred):
      logits, labels = eval_pred
      predictions = np.argmax(logits, axis=-1)
      accuracy = accuracy_score(labels, predictions)
      return {"accuracy": accuracy}

    training_args = TrainingArguments(
        output_dir="./results",
        per_device_eval_batch_size=batch_size,
        per_device_train_batch_size=batch_size,
        num_train_epochs=n_epochs,
        report_to="none",
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=None,
        eval_dataset=tokenized_test,
        compute_metrics=compute_metrics,
    )

    #Evaluation
    results = trainer.evaluate()
    print(f"eval accuracy: {results['eval_accuracy']}")
    return results["eval_accuracy"]

def evaluate_glue_task(model, tokenizer, task_name, batch_size=32, device=None):
    """
    Evaluates a classification model on a single GLUE task using its benchmark metric.

    Args:
        model: HuggingFace transformer model.
        tokenizer: Corresponding tokenizer.
        task_name: One of 'cola', 'mrpc', 'rte', 'qnli', 'wnli'.
        batch_size: Batch size for DataLoader.
        device: 'cuda' or 'cpu'. Defaults to auto-detection.

    Returns:
        Dictionary with the evaluation metric(s).
    """
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    #Determine text columns based on task
    if task_name == "cola":
        text1_key, text2_key = "sentence", None
    elif task_name == "mrpc":
        text1_key, text2_key = "sentence1", "sentence2"
    elif task_name == "qnli":
        text1_key, text2_key = "question", "sentence"
    elif task_name in ["rte", "wnli"]:
        text1_key, text2_key = "sentence1", "sentence2"
    else:
        raise ValueError(f"Unsupported GLUE task: {task_name}")

    #Load dataset and tokenize
    dataset = load_dataset("glue", task_name, split="validation")

    def tokenize_function(example):
        if text2_key:
            return tokenizer(example[text1_key], example[text2_key], truncation=True)
        return tokenizer(example[text1_key], truncation=True)

    dataset = dataset.map(tokenize_function)
    dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

    dataloader = DataLoader(dataset, batch_size=batch_size,
                            collate_fn=DataCollatorWithPadding(tokenizer))

    preds, labels = [], []

    with torch.no_grad():
        for batch in dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            labels_batch = batch.pop("labels")
            outputs = model(**batch)
            predictions = torch.argmax(outputs.logits, dim=-1)
            preds.extend(predictions.cpu().tolist())
            labels.extend(labels_batch.cpu().tolist())

    #Evaluate with appropriate metric
    if task_name == "cola":
        return {"matthews": matthews_corrcoef(labels, preds)}
    elif task_name == "mrpc":
        return {
            "accuracy": accuracy_score(labels, preds),
            "f1": f1_score(labels, preds)
        }
    elif task_name in ["rte", "qnli", "wnli"]:
        return {"accuracy": accuracy_score(labels, preds)}
    else:
        raise ValueError(f"Unsupported GLUE task: {task_name}")

def get_tokenized_dataset(Model, dataset_name, split, train_fraction=1, test_fraction=1):
    '''
    Returnes a tokenized dataset from the GLUE benchmark. The tokenization is done accordingly to the model passed as an argument

    Args:
        raw_Model (object): Model we want to fine-tune (should have .model attribute)
        dataset_name (str): Name of GLUE dataset ('cola', 'sst2', 'mrpc', 'stsb', 'qqp', 'mnli', 'qnli', 'rte', 'wnli')
        split (str) : e.g. 'train', 'validation', 'test'
        train_fraction (float): Fraction of training data to use (0-1)
        test_fraction (float): Fraction of validation data to use for testing (0-1)

    Returns:
        object: tokenized dataset
    '''

    model = Model.model
    tokenizer = Model.tokenizer
    dataset = load_dataset('glue', dataset_name)
    dataset = dataset[split].select(range(int(len(dataset[split]) * test_fraction))) #selects dataset split

    if dataset_name == "cola":
        def tokenize_function(examples):
            return tokenizer(examples['sentence'], padding="max_length", truncation=True)

    elif dataset_name == "sst2":
        def tokenize_function(examples):
            return tokenizer(examples['sentence'], padding="max_length", truncation=True)

    elif dataset_name == "mrpc":
        def tokenize_function(examples):
            return tokenizer(examples['sentence1'], examples['sentence2'],
                           padding="max_length", truncation=True)

    elif dataset_name == "qqp":
        def tokenize_function(examples):
            return tokenizer(examples['question1'], examples['question2'],
                           padding="max_length", truncation=True)

    elif dataset_name == "mnli":
        def tokenize_function(examples):
            return tokenizer(examples['premise'], examples['hypothesis'],
                           padding="max_length", truncation=True)

    elif dataset_name == "qnli":
        def tokenize_function(examples):
            return tokenizer(examples['question'], examples['sentence'],
                           padding="max_length", truncation=True)

    elif dataset_name == "rte":
        def tokenize_function(examples):
            return tokenizer(examples['sentence1'], examples['sentence2'],
                           padding="max_length", truncation=True)

    elif dataset_name == "wnli":
        def tokenize_function(examples):
            return tokenizer(examples['sentence1'], examples['sentence2'],
                           padding="max_length", truncation=True)
    else:
        raise ValueError(f"Unknown dataset name: {dataset_name}")

    #Tokenize datasets
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

    return tokenized_dataset

def sample_tokenized_dataset(tokenized_dataset, batch_size) :
  dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)
  batch = next(iter(dataloader))
  return batch