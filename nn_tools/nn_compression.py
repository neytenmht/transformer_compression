# -*- coding: utf-8 -*-
"""nn_compression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qG8OFPjoVPLZxel6cePi9I7AQ0XMKKYC
"""

'''
from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/MA2/Master Thesis/project')'''

import numpy as np
import matplotlib.pyplot as plt
import copy
import math
import time
import seaborn as sns

import torch
import torch.nn as nn
from torch.nn import Sequential
import torch.nn.functional as F

from datasets import load_dataset
from torch.utils.data import DataLoader

from transformers import (
    BertModel, BertForSequenceClassification, BertTokenizer, BertConfig,
    GPT2Model, GPT2ForSequenceClassification, GPT2Tokenizer, GPT2Config,
    RobertaModel, RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig,
)
from transformers import Trainer, TrainingArguments
from sklearn.metrics import accuracy_score

from nn_tools import algebraic as alg

class Model:

  def __init__(self, name, task, weight_path=None, **kwargs):
      '''
      Class that allows to load a model, and compute some operations on it

      Input parameters:
          name (str): e.g. "bert", "gpt2", "llama"
          task (str): e.g. "pretrained", "classification"
          weight_path (str): path to model weights (None for base model)
          kwargs: {
              "n_class" (int): number of classes for classification,
              "set_model" (bool): if providing PyTorch model directly,
              "model" (nn.Module): PyTorch model if set_model=True,
              "model_name" (str): specific model variant (e.g. "meta-llama/Llama-2-7b-chat-hf")
          }
      '''
      if name not in ["bert", "gpt2", "roberta"]:
          raise ValueError(f"Invalid name: {name}. Expected one of: 'bert', 'gpt2', 'roberta'")
      if task not in ["pretrained", "classification"]:
          raise ValueError(f"Invalid task: {task}. Expected one of: 'pretrained', 'classification'")

      self.name = name
      self.task = task
      self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
      self.tokenized_dataset = {}

      set_model = kwargs.get("set_model", False)
      self.model_name = kwargs.get("model_name", None)
      self.compression_blocks = []
      self.compression_layers = []

      if set_model:
          if "model" not in kwargs:
              raise ValueError("'model' must be provided when 'set_model' is True")
          self.model = kwargs["model"].to(self.device)
      else:
          #BERT
          if self.name == "bert":
              self._init_bert(weight_path, kwargs)

          #GPT-2
          elif self.name == "gpt2":
              self._init_gpt2(weight_path, kwargs)

           #ROBERTA
          elif self.name == "roberta":
            self._init_roberta(weight_path, kwargs)

      self.svd_dict = {}

  def _init_bert(self, weight_path, kwargs):
      self.n_layers = 12
      if weight_path is not None:
          self.model = torch.load(weight_path, weights_only=False)
      else:
          if self.task == "pretrained":
              from transformers import BertModel
              self.model = BertModel.from_pretrained('bert-base-uncased').to(self.device)

          elif self.task == "classification":
              from transformers import BertConfig, BertForSequenceClassification
              config = BertConfig.from_pretrained('bert-base-uncased',
                                                num_labels=kwargs["n_class"])
              self.model = BertForSequenceClassification.from_pretrained(
                  'bert-base-uncased', config=config).to(self.device)

      self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
      self.compression_blocks = ["attn query", "attn key", "attn value", "attn output",  "intermediate", "output"]

      if self.task == "pretrained":
        for i in range(self.n_layers) :
          self.compression_layers.append(f"encoder.layer.{str(i)}.attention.self.key")
          self.compression_layers.append(f"encoder.layer.{str(i)}.attention.self.query")
          self.compression_layers.append(f"encoder.layer.{str(i)}.attention.self.value")
          self.compression_layers.append(f"encoder.layer.{str(i)}.attention.output.dense")
          self.compression_layers.append(f"encoder.layer.{str(i)}.intermediate.dense")
          self.compression_layers.append(f"encoder.layer.{str(i)}.output.dense")
      else :
        for i in range(self.n_layers) :
          self.compression_layers.append(f"bert.encoder.layer.{str(i)}.attention.self.key")
          self.compression_layers.append(f"bert.encoder.layer.{str(i)}.attention.self.query")
          self.compression_layers.append(f"bert.encoder.layer.{str(i)}.attention.self.value")
          self.compression_layers.append(f"bert.encoder.layer.{str(i)}.attention.output.dense")
          self.compression_layers.append(f"bert.encoder.layer.{str(i)}.intermediate.dense")
          self.compression_layers.append(f"bert.encoder.layer.{str(i)}.output.dense")

  def _init_gpt2(self, weight_path, kwargs):
      self.n_layers = 12
      if weight_path is not None:
          self.model = torch.load(weight_path, weights_only=False)
      else:
          if self.task == "pretrained":
              self.model = GPT2Model.from_pretrained('gpt2').to(self.device)

          elif self.task == "classification":
              config = GPT2Config.from_pretrained('gpt2',
                                                num_labels=kwargs["n_class"])
              self.model = GPT2ForSequenceClassification.from_pretrained(
                  'gpt2', config=config).to(self.device)

      self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
      if self.tokenizer.pad_token is None:
          self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
          self.model.resize_token_embeddings(len(self.tokenizer))
          self.model.config.pad_token_id = self.tokenizer.pad_token_id

      self.compression_blocks = ["attn.c_attn", "attn.c_proj", "mlp.c_fc", "mlp.c_proj"]

      if self.task == "pretrained":
        for i in range(self.n_layers) :
          self.compression_layers.append(f"h.{str(i)}.attn.c_attn")
          self.compression_layers.append(f"h.{str(i)}.attn.c_proj")
          self.compression_layers.append(f"h.{str(i)}.mlp.c_fc")
          self.compression_layers.append(f"h.{str(i)}.mlp.c_proj")
      else :
        for i in range(self.n_layers) :
          self.compression_layers.append(f"transformer.h.{str(i)}.attn.c_attn")
          self.compression_layers.append(f"transformer.h.{str(i)}.attn.c_proj")
          self.compression_layers.append(f"transformer.h.{str(i)}.mlp.c_fc")
          self.compression_layers.append(f"transformer.h.{str(i)}.mlp.c_proj")

  def _init_roberta(self, weight_path, kwargs):
    self.n_layers = 12
    if weight_path is not None:
        self.model = torch.load(weight_path, weights_only=False)
    else:
        if self.task == "pretrained":
            self.model = RobertaModel.from_pretrained('roberta-base').to(self.device)
        elif self.task == "classification":
            config = RobertaConfig.from_pretrained('roberta-base',
                                              num_labels=kwargs["n_class"])
            self.model = RobertaForSequenceClassification.from_pretrained(
                'roberta-base', config=config).to(self.device)
    self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
    self.compression_blocks = ["attn query", "attn key", "attn value", "attn output",  "intermediate", "output"]

    if self.task == "pretrained":
      for i in range(self.n_layers) :
        self.compression_layers.append(f"encoder.layer.{str(i)}.attention.self.key")
        self.compression_layers.append(f"encoder.layer.{str(i)}.attention.self.query")
        self.compression_layers.append(f"encoder.layer.{str(i)}.attention.self.value")
        self.compression_layers.append(f"encoder.layer.{str(i)}.attention.outpu.denset")
        self.compression_layers.append(f"encoder.layer.{str(i)}.intermediate.dense")
        self.compression_layers.append(f"encoder.layer.{str(i)}.output.dense")
    else :
      for i in range(self.n_layers) :
        self.compression_layers.append(f"roberta.encoder.layer.{str(i)}.attention.self.key")
        self.compression_layers.append(f"roberta.encoder.layer.{str(i)}.attention.self.query")
        self.compression_layers.append(f"roberta.encoder.layer.{str(i)}.attention.self.value")
        self.compression_layers.append(f"roberta.encoder.layer.{str(i)}.attention.output.dense")
        self.compression_layers.append(f"roberta.encoder.layer.{str(i)}.intermediate.dense")
        self.compression_layers.append(f"roberta.encoder.layer.{str(i)}.output.dense")


  def parse_layer_name(self, layer_name):
    '''
    Args:
        layer_name (str): Complete layer name (e.g., "encoder.layer.5.attention.self.key")

    Returns:
        tuple: (block_type, layer_idx) where block_type is one of "key", "query", "value", "inter", "output"
               and layer_idx is the integer layer index
    '''

    if self.name == "bert" :
      #Split the layer name into components
      parts = layer_name.split('.')

      if parts[0] == "bert":
          parts = parts[1:]  
      try:
          #Find the layer index
          layer_idx_pos = parts.index("layer") + 1
          layer_idx = int(parts[layer_idx_pos])
      except (ValueError, IndexError):
          raise ValueError(f"Could not parse layer index from {layer_name}")

      #Determine the block type based on the suffix
      suffix = '.'.join(parts[layer_idx_pos+1:])

      if "attention.self.key" in suffix:
          block_type = "attn key"
      elif "attention.self.query" in suffix:
          block_type = "attn query"
      elif "attention.self.value" in suffix:
          block_type = "attn value"
      elif "attention.output.dense" in suffix:
          block_type = "attn output"
      elif "intermediate.dense" in suffix:
          block_type = "intermediate"
      elif "output.dense" in suffix:
          block_type = "output"
      else:
          raise ValueError(f"Could not determine block type from {layer_name}")

      return block_type, layer_idx

    #GPT-2
    elif self.name == "gpt2" :
      if self.task == "pretrained" :
        parts = layer_name.split('.')
        block_type = ".".join(parts[2:])
        layer_idx = int(parts[1])
        return block_type, layer_idx
      else :
        parts = layer_name.split('.')
        block_type = ".".join(parts[3:])
        layer_idx = int(parts[2])
        return block_type, layer_idx

    #Roberta
    elif self.name == "roberta" :
      #Split the layer name into components
      parts = layer_name.split('.')
      
      if parts[0] == "roberta":
          parts = parts[1:]  

      #Find the layer index
      try:
          layer_idx_pos = parts.index("layer") + 1
          layer_idx = int(parts[layer_idx_pos])
      except (ValueError, IndexError):
          raise ValueError(f"Could not parse layer index from {layer_name}")

      #Determine the block type based on the suffix
      suffix = '.'.join(parts[layer_idx_pos+1:])

      if "attention.self.key" in suffix:
          block_type = "attn key"
      elif "attention.self.query" in suffix:
          block_type = "attn query"
      elif "attention.self.value" in suffix:
          block_type = "attn value"
      elif "attention.output.dense" in suffix:
          block_type = "attn output"
      elif "intermediate.dense" in suffix:
          block_type = "intermediate"
      elif "output.dense" in suffix:
          block_type = "output"
      else:
          raise ValueError(f"Could not determine block type from {layer_name}")

      return block_type, layer_idx



  def weight_matrix(self, layer_name):
    '''
      Returns the weight matrix corresponding to a given block and layer

      Args :
        layer_name (str) : e.g.

      Returns
        (pytorch tensor) : weight matrix correspong to given block at given layer
    '''


    return self.model.state_dict()[layer_name + str(".weight")]



  def layer_forward_pass(self, input, layer_name) :
    '''
      Computes forward pass on a single target layer

      Args :
        input (pytorch tensor [batch, seq_len, hidden_size]) : hidden_size usually equals 768
        layer_name (str) : e.g.

      Returns :
        (pytorch tensor) : [batch, seq_len, output_size] weight matrix correspong to given block at given layer
    '''

    layer_path = layer_name.split('.')
    layer = self.model

    for part in layer_path:
        if part.isdigit():
            part = int(part)
            layer = layer[part]
        else:
            layer = getattr(layer, part)

    with torch.no_grad():
        output = layer(input)

    return output


  def forward_pass(self, input, skip_embedding_layer=True, cls_output=False) :
    '''
      Computes forward pass on the entire model (except word embedding layer if skip_embedding_layer==True)

      Args :
        input (pytorch tensor) : [batch, seq_len, hidden_size] hidden_size usually equals 768
        skip_embedding_layer (bool) : if we skip the word embedding layer during forward pass

      Returns :
        (pytorch tensor) : output of the model
    '''

    self.model.config.output_hidden_states = True
    self.model.eval()

    with torch.no_grad():

      if skip_embedding_layer == True :
          if self.name == "bert" :
            if self.task == "pretrained" : #pretrained architecture
              output = self.model.encoder(input).last_hidden_state
              self.model.config.output_hidden_states = False
              return output

            elif self.task == "classification" : #classification architecture
              output = self.model.bert.encoder(input).last_hidden_state
              self.model.config.output_hidden_states = False
              if cls_output==False : #no classification output
                return output
              else : #classification output
                pooled_output = self.model.bert.pooler(output)
                logits = self.model.classifier(pooled_output)
                return logits

          #GPT-2 
          elif self.name == "gpt2":
            if self.task == "pretrained" : #pretrained architecture
              hidden_states = input
              #Pass through each transformer block sequentially
              for block in self.model.h:
                  hidden_states = block(hidden_states)[0]
              #Apply final layer norm
              output = self.model.ln_f(hidden_states)
              self.model.config.output_hidden_states = False
              return output

            elif self.task == "classification":
                hidden_states = input
                #Pass through each transformer block sequentially
                for block in self.model.transformer.h:
                    hidden_states = block(hidden_states)[0]
                #Apply final layer norm
                output = self.model.transformer.ln_f(hidden_states)
                self.model.config.output_hidden_states = False
                #For classification, use the last token's hidden state
                last_token_states = output[:, -1, :]
                logits = self.model.score(last_token_states)
                if cls_output==False : #no classification output
                    return output
                else:
                    return logits


          #ROBERTA implementation
          elif self.name == "roberta" :
            if self.task == "pretrained" : #pretrained architecture
              output = self.model.encoder(input).last_hidden_state
              self.model.config.output_hidden_states = False
              return output

            elif self.task == "classification" : #classification architecture
              output = self.model.roberta.encoder(input).last_hidden_state
              self.model.config.output_hidden_states = False
              if cls_output==False : #no classification output
                return output
              else : #classification output
                logits = self.model.classifier(output)
                return logits


      else :
        if self.name == "bert" :
          input_ids = input['input_ids'].to(self.device)          
          attention_mask = input['attention_mask'].to(self.device) 
          labels = input['label'].to(self.device)                

          with torch.no_grad():  
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True  
            )

          output = outputs.hidden_states[-1]  #(batch_size, seq_len, hidden_size)
          self.model.config.output_hidden_states = False

          if cls_output==False :
            return output
          else : #classification output
            pooled_output = self.model.bert.pooler(output)
            logits = self.model.classifier(pooled_output)
            return logits

        #GPT-2 
        elif self.name == "gpt2":
            input_ids = input['input_ids'].to(self.device)
            attention_mask = input['attention_mask'].to(self.device)
            labels = input['label'].to(self.device)  

            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True
            )

            self.model.config.output_hidden_states = False

            if self.task == "pretrained":
                return outputs.last_hidden_state
            elif self.task == "classification":
                if cls_output:
                    return outputs.logits
                else:
                    return outputs.hidden_states[-1]

        #ROBERTA 
        elif self.name =='roberta' :
          input_ids = input['input_ids'].to(self.device)          
          attention_mask = input['attention_mask'].to(self.device) 
          labels = input['label'].to(self.device)                

          with torch.no_grad():  
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True  
            )

          output = outputs.hidden_states[-1]  #(batch_size, seq_len, hidden_size)
          self.model.config.output_hidden_states = False

          if cls_output==False :
            return output
          else : #classification output
            logits = self.model.classifier(output)
            return logits


  def model_svd(self) :
    """
      Compute full SVD factorization of the model's weight matrices

      Args:
          compressed_blocks (str array) : type of blocks we want to compute the SVD for (e.g. ['output', 'key', 'query', 'value', 'intermediate'])
      """

    if len(self.svd_dict) == 0: 
      for layer_name in self.compression_layers :
          if self.name == 'gpt2': #
          #Conv1D: weight is (in_features, out_features) -> transpose to (out_features, in_features)
              W = self.weight_matrix(layer_name).cpu().detach().numpy().T # Transpose here!
          else:
              W = self.weight_matrix(layer_name).cpu().detach().numpy()
          self.svd_dict[layer_name] = {}
          U, S, V = np.linalg.svd(W, full_matrices=False)
          self.svd_dict[layer_name]["U"] = U
          self.svd_dict[layer_name]["S"] = S
          self.svd_dict[layer_name]["V"] = V


  def replace_layer_with_low_rank(self, layer_name, rank_ratio, method='svd', dataset_name=None, proxy_dataset_size=32):
    if method == 'svd':
        """
        Replace a given layer in a copied model with the product of two low-rank layers

        Args:
            model (torch.nn.Module): The pre-trained model to copy and modify
            layer_name (str): Name of the layer to compress
            rank_ratio (float): The rank ratio for the low-rank approximation
        """

        #Create a deep copy of the model and access the target layer
        modified_model = copy.deepcopy(self.model)
        module_parts = layer_name.split('.')
        parent_module = modified_model

        for part in module_parts[:-1]:
            parent_module = getattr(parent_module, part)

        target_layer_name = module_parts[-1]
        original_layer = getattr(parent_module, target_layer_name)

        #check if original layer has a bias term
        has_bias = original_layer.bias is not None

        if self.name == 'gpt2':
            #Handle GPT-2's Conv1D layer weights
            W = original_layer.weight.data.T  # Transpose for GPT-2
        else:
            W = original_layer.weight.data  #(out_features, in_features)

        #Get bias if exists
        if has_bias:
            bias = original_layer.bias.data
        else:
            bias = None

        #SVD
        full_r = min(W.shape)
        rank = int(full_r * rank_ratio)

        if layer_name in self.svd_dict:
            U = torch.tensor(self.svd_dict[layer_name]["U"]).to(self.device)
            S = torch.tensor(self.svd_dict[layer_name]["S"]).to(self.device)
            Vh = torch.tensor(self.svd_dict[layer_name]["V"]).to(self.device)
        else:
            U, S, Vh = torch.linalg.svd(W, full_matrices=False)

        U_r = U[:, :rank]       
        S_r = S[:rank]          
        Vh_r = Vh[:rank, :]     


        #Factorized layer 
        class LowRankLayer(nn.Module):
          def __init__(self, fc1_A, fc1_B):
              super().__init__()
              self.fc1_A = fc1_A  
              self.fc1_B = fc1_B  

          def forward(self, x):
              x = self.fc1_A(x)
              x = self.fc1_B(x)
              return x

        #Create low-rank factorized layers
        fc1_A = nn.Linear(Vh_r.shape[1], rank, bias=False)
        fc1_B = nn.Linear(rank, U_r.shape[0], bias=has_bias)

        #Set weights
        fc1_A.weight.data = Vh_r
        fc1_B.weight.data = U_r * S_r
        if has_bias:
            fc1_B.bias.data = bias

        #Replace with factorized layer layer instead of Sequential
        new_layer = LowRankLayer(fc1_A, fc1_B)
        setattr(parent_module, target_layer_name, new_layer)
        self.model = modified_model

    elif method == 'afm':
      #Extract layer we want to replace
      modified_model = copy.deepcopy(self.model)
      module_parts = layer_name.split('.')
      parent_module = modified_model
      for part in module_parts[:-1]:
          parent_module = getattr(parent_module, part)
      target_layer_name = module_parts[-1]
      original_layer = getattr(parent_module, target_layer_name)

      #Check for bias existence
      has_bias = original_layer.bias is not None

      #Get layer dimensions
      if self.name == "gpt2":
          in_features = original_layer.nx
          out_features = original_layer.nf
      else:
          in_features = original_layer.in_features
          out_features = original_layer.out_features

      #Weight extraction
      if self.name == "gpt2":
          W = original_layer.weight.data
      else:
          A = original_layer.weight.data
          W = A.T

      #Handle bias
      if has_bias:
          b = original_layer.bias.data
      else:
          b = torch.zeros(out_features).to(self.device)  # Zero-initialized bias

      #Generate proxy dataset
      proxy_dataset = self.sample_tokenized_dataset(dataset_name, proxy_dataset_size)
      N = len(proxy_dataset['input_ids'][0])

      #Initialize statistics tensors
      E_y = torch.zeros(N, out_features).to(self.device)
      E_yTy = torch.zeros(out_features, out_features).to(self.device)

      #Hook function with bias handling
      def extract_original_layer_output(input_tensor, original_layer):
          def hook(module, input, output):
              nonlocal y
              y = output[0] + (module.bias if module.bias is not None else 0)
          hook_handle = original_layer.register_forward_hook(hook)
          modified_model(**input_tensor)
          hook_handle.remove()
          return y

      #Compute expected value and covariance of the output 
      for i in range(len(proxy_dataset['input_ids'])):
          inputs = {
              "input_ids": proxy_dataset['input_ids'][i].unsqueeze(0).to(self.device),
              "attention_mask": proxy_dataset['attention_mask'][i].unsqueeze(0).to(self.device),
          }
          if 'token_type_ids' in proxy_dataset:
              inputs["token_type_ids"] = proxy_dataset['token_type_ids'][i].unsqueeze(0).to(self.device)

          with torch.no_grad():
              y = extract_original_layer_output(inputs, original_layer)
              y = y.squeeze(0)
              E_y += y
              E_yTy += torch.matmul(y.T, y)

      E_y /= proxy_dataset_size
      E_yTy /= proxy_dataset_size
      Cov_yT = E_yTy - torch.matmul(E_y.T, E_y)

      #PCA
      U, S, Vt = torch.svd(Cov_yT)
      k = int(rank_ratio * min(in_features, out_features))
      Uk = U[:, :k]

      #Parameters of the factorized low-rank layer
      W1 = torch.matmul(W, Uk)
      b1 = torch.matmul(b, Uk) if has_bias else torch.zeros(k).to(self.device)
      W2 = Uk.T
      b2 = (E_y - torch.matmul(E_y, torch.matmul(Uk, Uk.T))).mean(dim=0)

      class AFMLowRankLayer(nn.Module):
        def __init__(self, in_features, out_features, k, W1, W2, b1, b2, has_bias):
            super().__init__()
            self.fc1 = nn.Linear(in_features, k, bias=True)
            self.fc2 = nn.Linear(k, out_features, bias=True)

            self.fc1.weight.data = W1.T
            self.fc1.bias.data = b1
            self.fc2.weight.data = W2.T
            self.fc2.bias.data = b2

        def forward(self, x):
            return self.fc2(self.fc1(x))

      #Create and replace layer
      new_layer = AFMLowRankLayer(in_features, out_features, k, W1, W2, b1, b2, has_bias)
      setattr(parent_module, target_layer_name, new_layer)
      self.model = modified_model

    else :
      raise NameError("method name should be either 'svd' or 'afm'")


  def tokenize_dataset(self, dataset_name, split, train_fraction=1, test_fraction=1) :
    '''
    Returnes a tokenized dataset from the GLUE benchmark. The tokenization is done accordingly to the model passed as an argument

    Args:
        raw_Model (object): Model we want to fine-tune (should have .model attribute)
        dataset_name (str): Name of GLUE dataset ('cola', 'sst2', 'mrpc', 'stsb', 'qqp', 'mnli', 'qnli', 'rte', 'wnli')
        split (str) : e.g. 'train', 'validation', 'test'
        train_fraction (float): Fraction of training data to use (0-1)
        test_fraction (float): Fraction of validation data to use for testing (0-1)

    Returns:
        object: tokenized dataset
    '''

    '''
      MIGHT DELET, not up-to-date and useful
    '''

    model = self.model
    tokenizer = self.tokenizer
    dataset = load_dataset('glue', dataset_name)
    dataset = dataset[split].select(range(int(len(dataset[split]) * test_fraction))) #selects dataset split

    def get_tokenize_function(dataset_name, model_name, tokenizer):
      """Returns the appropriate tokenization function for the dataset and model type"""
      #First define the column names for each dataset
      dataset_columns = {
          "cola": ("sentence", None),
          "sst2": ("sentence", None),
          "mrpc": ("sentence1", "sentence2"),
          "qqp": ("question1", "question2"),
          "mnli": ("premise", "hypothesis"),
          "qnli": ("question", "sentence"),
          "rte": ("sentence1", "sentence2"),
          "wnli": ("sentence1", "sentence2")
      }

      if dataset_name not in dataset_columns:
          raise ValueError(f"Unknown dataset name: {dataset_name}")

      col1, col2 = dataset_columns[dataset_name]


      def tokenize_function(examples):
          if col2:
              return tokenizer(
                  examples[col1],
                  examples[col2],
                  padding="max_length",
                  truncation=True,
                  max_length=128,
                  return_tensors="pt"
              )
          else:
              return tokenizer(
                  examples[col1],
                  padding="max_length",
                  truncation=True,
                  max_length=128,
                  return_tensors="pt"
              )

      return tokenize_function

    tokenize_function = get_tokenize_function(dataset_name, self.name, tokenizer)
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

    self.tokenized_dataset[dataset_name]  = tokenized_dataset

  def sample_tokenized_dataset(self, dataset_name, batch_size) :
    '''DEAL WITH PROXY WHEN RANDOM BC NO NEED TO HAVE 'input_ids' etc...'''

    if dataset_name == 'random' :
      batch = np.random.normal(0, 1, (batch_size, 10, 768))
      batch = torch.tensor(batch, dtype=torch.float32).to(self.device)
    else :
      dataloader = DataLoader(self.tokenized_dataset[dataset_name], batch_size=batch_size, shuffle=True)
      batch = next(iter(dataloader))
    return batch

def layer_random_output_error(original_model, compressed_model, block, layer_idx, batch, seq_len, hidden_size, mu, sigma) :
  '''
    Computes the output of a base model and compare it to the output of its compressed version
    The input is random (entries are i.i.d from N(mu, sigma)) and passes through a single layer (block, layer_idx)

    Args :
      original_model (torch.nn.Module) : full version of the model
      compressed_model (torch.nn.Module) : compressed version of the model
      block (str) : architectural block type e.g. "key"
      layer_idx (int) : layer index
      batch (int) : batch size of the random input
      seq_len (int) : sequence length of the random input
      hidden_size (int) : hidden size of the random input
      mu, sigma (int) : parameters of the normal distribution

    Returns :
      (float) : normalized frobenius norm between the 2 outputs
  '''

  if original_model.device == compressed_model.device :
    input = np.random.normal(mu, sigma, (batch, seq_len, hidden_size))
    input = torch.tensor(input, dtype=torch.float32).to(original_model.device)

    Y = original_model.layer_forward_pass(input, block, layer_idx).detach().cpu().numpy()
    Y = Y.reshape((batch, seq_len*Y.shape[2]))
    Y_ = compressed_model.layer_forward_pass(input, block, layer_idx).detach().cpu().numpy()
    Y_ = Y_.reshape((batch, seq_len*Y_.shape[2]))

    return np.linalg.norm(Y - Y_, 'fro')/np.linalg.norm(Y, 'fro')

def get_output_error(original_Model, compressed_Model, proxy_type, dataset_name=None, batch=16, seq_len=10, hidden_size=768, mu=0, sigma=1, error_type="hidden_output", base_accuracy = None, dataset=None, alpha=0.5) :
  '''
    Computes the output of a base model and compare it to the output of its compressed version
    The input is random (entries are i.i.d from N(mu, sigma)) and passes through the entire model

    Args :
      original_model (torch.nn.Module) : full version of the model
      compressed_model (torch.nn.Module) : compressed version of the model
      block (str) : architectural block type e.g. "key"
      layer_idx (int) : layer index
      batch (int) : batch size of the random input
      proxy_type (str) : e.g. 'proxy' or 'dataset'
      tokenized_sample (Dataset object) : tokenized sample of shape (batch, seq_len, features)
      seq_len (int) : sequence length of the random input
      hidden_size (int) : hidden size of the random input
      mu, sigma (int) : parameters of the normal distribution

    Returns :
      (float) : normalized frobenius norm between the 2 outputs
  '''
  if original_Model.device == compressed_Model.device :
    if proxy_type == 'random' :
      input = np.random.normal(mu, sigma, (batch, seq_len, hidden_size))
      input = torch.tensor(input, dtype=torch.float32).to(original_Model.device)
      if error_type == 'hidden_output' :
        Y = original_Model.forward_pass(input, skip_embedding_layer=True).detach().cpu().numpy()
        Y_ = compressed_Model.forward_pass(input, skip_embedding_layer=True).detach().cpu().numpy()
      elif error_type == 'cls_output' :
        Y = original_Model.forward_pass(input, skip_embedding_layer=True, cls_output=True).detach().cpu().numpy()
        Y_ = compressed_Model.forward_pass(input, skip_embedding_layer=True, cls_output=True).detach().cpu().numpy()
      else :
        raise NameError("error type is either 'cls_output' or 'hidden_output'")

    elif proxy_type == 'dataset' :
      input = original_Model.sample_tokenized_dataset(dataset_name, batch)

      if error_type == 'hidden_output' :
        Y = original_Model.forward_pass(input, skip_embedding_layer=False).detach().cpu().numpy()
        Y_ = compressed_Model.forward_pass(input, skip_embedding_layer=False).detach().cpu().numpy()

      elif error_type == 'distillation' :
        logits = original_Model.forward_pass(input, skip_embedding_layer=False, cls_output=True)
        preds = torch.argmax(logits, dim=-1)
        Y = F.one_hot(preds, num_classes=logits.size(-1)).detach().cpu().numpy()
        logits = compressed_Model.forward_pass(input, skip_embedding_layer=False, cls_output=True)
        preds = torch.argmax(logits, dim=-1)
        Y_ = F.one_hot(preds, num_classes=logits.size(-1)).detach().cpu().numpy()
        hard_err = np.linalg.norm(Y - Y_, 'fro')/np.linalg.norm(Y, 'fro')

        Y = original_Model.forward_pass(input, skip_embedding_layer=False, cls_output=True).detach().cpu().numpy()
        Y_ = compressed_Model.forward_pass(input, skip_embedding_layer=False, cls_output=True).detach().cpu().numpy()
        Y = Y.reshape((batch, -1))
        Y_ = Y_.reshape((batch, -1))
        soft_err = np.linalg.norm(Y - Y_, 'fro')/np.linalg.norm(Y, 'fro')

        return (1-alpha)*soft_err + alpha*hard_err
      else :
        raise NameError("error type is either 'hidden_output' or 'distillation")

    else :
      raise NameError("proxy type is either 'random' or 'dataset'")

    Y = Y.reshape((batch, -1))
    Y_ = Y_.reshape((batch, -1))
    return np.linalg.norm(Y - Y_, 'fro')/np.linalg.norm(Y, 'fro')

  else :
    raise ValueError('original model and compressed model have to be on the same device')

def knapsack_greedy_compression(Model,
                                min_rank_ratios=np.linspace(0.1, 0.6, 6), max_rank_ratio=0.7, rank_ratio_step=0.1, output_threshold=0.0015,
                                proxy_type = 'random', dataset_name = None, batch=16, seq_len=10, hidden_size=768, mu=0, sigma=1,
                                complexity_metric = 'n_param', lr_metric = 'sv_area', error_type='hidden_output', method='svd', base_accuracy = None, verbose=False, alpha=0.5, rank_only=False):

    '''
      Find the biggest compression ratio such that the output error is below a given threshold
        Let M(.) be the original model, Mc(.) the compressed model and D a proxy dataset
        The output error is computed as norm(Y-Y_), where Y=M(D) and Y_=Mc(D) and has to be s.t. norm(Y-Y_) <= output_threshold

        The algorithm sorts the weight matrices of the model by order of interest. The interest is computed as the product between the model complexity and low-rankness.
        Once the weight matrices are sorted, they are compressed according to a given rank ratio (i.e. proportion of columns kept after SVD).
        If the output threshold is reached, the rank ratio is increased until the output error is below the output threshold

        The procedure is repeated for several initial rank ratios


      Model (obj. Model) : Model object representing the model to be compressed
      compressed_blocks (arr. str) : array containing the block types to be compressed (e.g. ['output', 'key', 'query', 'value', 'intermediate'])
      min_rank_ratios (arr. float) : initial rank ratio's trails
      max_rank_ratio (float) : procedure stops when the rank ratio reached this value
      rank_ratio (float) : step by which the rank ratio is increased to match the output threshold
      output threshold (float) : s.t. norm(Y-Y_) <= output_threshold
      batch, seq_len (float, float) : batch and seq_length of the proxy inputs fed to the model
      hidden_size (float) : hidden size of the model, used to compute the random output error
      mu, sigma (float, float) : distribution parameters of the random proxy
      complexity metric (str) : metric used to compute complexity of a weight matrix (e.g. 'n_param')
      lr_metric (str) : metric used to approach the rank (e.g. 'sv_area' or 'hard_threshold')
      proxy_type (str) : type of the procy used to compute the output error (e.g. 'random', 'dataset')
      proxy_dataset (pytorch dataset) : in case proxy_type == 'dataset', dataset used as a proxy to compute the output error
    '''


    max_rank_reached = False
    best_min_rank_ratio = 0
    best_resulting_param_ratio = 1
    best_Model = copy.deepcopy(Model)

    ds = None
    if proxy_type == 'dataset' :
      ds = load_dataset("glue", dataset_name)

    # Initialize dictionary to store rank ratios for each block and layer
    rank_ratio_dict = {}
    best_rank_ratio_dict = {}

    if verbose :
      print("computing SVD of the model's layers... \n")
    Model.model_svd()



    weight_list = []
    for layer_name in Model.compression_layers:
        param = Model.weight_matrix(layer_name)

        if complexity_metric == 'n_param':
            complexity = param.shape[0] * param.shape[1]
        elif complexity_metric == 'max_param':
            complexity = max(param.shape[0], param.shape[1])
        else:
            raise ValueError("lr_metric has to be 'n_param'")

        rank_like_metric = alg.rank_metric(param.detach().cpu().numpy(), metric=lr_metric)

        if rank_only:
            value = (1 / rank_like_metric) * 1e7
        else:
            value = complexity / max(rank_like_metric, 1)
        weight_list.append((value, layer_name))

        #Initialize the rank ratio dictionary structure (for visualization)
        for block_name in Model.compression_blocks:
            rank_ratio_dict[block_name] = np.ones(Model.n_layers)
            best_rank_ratio_dict[block_name] = np.ones(Model.n_layers)

    #Sort the list by value (descending) and layer name (ascending for tie-breaking)
    weight_list.sort(key=lambda x: (-x[0], x[1]))
    sorted_weight_names = [layer_name for _, layer_name in weight_list]

    if verbose:
        print("Sorted layers by priority:")
        for value, name in weight_list:
            print(f"{name}: {value:.2f}")
        print(len(weight_list))
        print(f"\nTotal layers: {len(Model.compression_layers)}")

    if verbose :
      print('start of the compression algorithm \n')
    #Iterate over min_rank_ratios
    for min_rank_ratio in min_rank_ratios:
        if verbose :
          print(f'min rank ratio : {min_rank_ratio}')

        #Reset rank ratio dictionary for this iteration
        rank_ratio_dict = {block: np.ones(Model.n_layers) for block in rank_ratio_dict.keys()}

        current_rank_ratio = min_rank_ratio
        max_rank_reached = False
        lr_Model = copy.deepcopy(Model)

        for layer_name in sorted_weight_names:
            block_type, layer_idx = Model.parse_layer_name(layer_name)

            if max_rank_reached:
                '''
                # If max rank reached, keep the current rank ratio for remaining layers
                block_name = [b for b in compressed_blocks if b in layer_name][0]
                rank_ratio_dict[block_name].append(current_rank_ratio)'''
                continue

            temp_Model = copy.deepcopy(lr_Model)
            temp_Model.replace_layer_with_low_rank(layer_name, current_rank_ratio, method=method, dataset_name=dataset_name, proxy_dataset_size=batch)
            output_error = get_output_error(Model, temp_Model, proxy_type, dataset_name=dataset_name, batch=16, seq_len=seq_len, hidden_size=hidden_size, mu=mu, sigma=sigma, error_type=error_type, base_accuracy = base_accuracy, dataset=ds, alpha=alpha)
            if verbose :
              print(f'output_error : {output_error}')

            while output_error > output_threshold and not max_rank_reached:
                current_rank_ratio += rank_ratio_step
                if current_rank_ratio > max_rank_ratio:
                    max_rank_reached = True
                    break

                temp_Model = copy.deepcopy(lr_Model)
                temp_Model.replace_layer_with_low_rank(layer_name, current_rank_ratio, method=method, dataset_name=dataset_name, proxy_dataset_size=batch)
                output_error = get_output_error(Model, temp_Model, proxy_type, dataset_name=dataset_name, batch=16, seq_len=seq_len, hidden_size=hidden_size, mu=mu, sigma=sigma, error_type=error_type, base_accuracy = base_accuracy, dataset = ds, alpha=alpha)

            if verbose :
              print(f'current_rank_ratio : {current_rank_ratio}')
              print(f'output_error : {output_error} \n')

            if not max_rank_reached:
                lr_Model = copy.deepcopy(temp_Model)
                rank_ratio_dict[block_type][layer_idx] = current_rank_ratio


        resulting_param_ratio = count_parameters(lr_Model.model) / count_parameters(Model.model)
        if verbose :
          print(f'resulting_param_ratio : {resulting_param_ratio}')
        if resulting_param_ratio < best_resulting_param_ratio:
            best_resulting_param_ratio = resulting_param_ratio
            best_min_rank_ratio = min_rank_ratio
            best_Model = copy.deepcopy(lr_Model)
            best_rank_ratio_dict = copy.deepcopy(rank_ratio_dict)

    if verbose :
      print(f'best initial compression ratio : {best_min_rank_ratio}')
      print(f'best resulting relative size : {best_resulting_param_ratio}')

    return best_Model, best_rank_ratio_dict, best_resulting_param_ratio

def hard_thresholding_compression(tuned_Model, compressed_blocks, verbose=False) :

  Model = copy.deepcopy(tuned_Model)

  for name, param in Model.model.state_dict().items():
    if 'weight' in name and any(substring in name for substring in compressed_blocks):
      if len(param.shape) == 2 :
        layer_name = name[:-7]
        W = param.cpu().detach().numpy()
        sv_ = np.linalg.svd(W, compute_uv=False)
        med_sv = np.median(sv_)
        beta = W.shape[0]/W.shape[1]
        tau = alg.optimal_SVHT_coef(beta, False)*med_sv

        if verbose :
          print(f'layer_name : {layer_name}')
          print(f'beta : {beta}')

          plt.axhline(y=tau, color='r', linestyle='--', label=f'optimal threshold, beta : {beta}')
          plt.plot(sv_, label='singular value distribution')
          plt.xlabel('singular value index')
          plt.ylabel('singular value magnitude')
          plt.title('sorted singular values magnitude and optimal threshold')
          plt.legend()
          plt.show()

        rank_ratio = proportion_above_threshold(sv_, tau)
        if rank_ratio != 0 :
          Model.replace_layer_with_low_rank(layer_name, rank_ratio)

  return Model

def fool_svd_compression(tuned_Model, rank_ratio=1) :

    print("computing SVD of the model's layers... \n")
    Model = copy.deepcopy(tuned_Model)
    Model.model_svd()

    for layer_name in Model.compression_layers :
        Model.replace_layer_with_low_rank(layer_name, rank_ratio)
    Model.svd_dict = {}
    return Model

def count_parameters(model):
  '''
    counts the number of learnable parameters in a model
  '''

  return sum(p.numel() for p in model.parameters() if p.requires_grad)

def plot_rank_ratio_heatmap(rank_ratio_dict, title, uniform_scale=True, vmin=0, vmax=1):
    """
    Display compression configuration as a heatmap, with an option for a uniform color scale.

    Parameters
    ----------
    rank_ratio_dict : dict
        keys are architectural blocks, values are lists/arrays of length ≤ n_layers
        each element corresponds to the compression ratio for that layer index.
    title : str
        the plot title.
    uniform_scale : bool, default True
        if True, computes vmin/vmax across all values in rank_ratio_dict and uses that
        for the colorbar. If False, lets seaborn auto‐scale per plot.
    vmin : float, optional
        override the minimum value for the color scale (takes precedence over computed).
    vmax : float, optional
        override the maximum value for the color scale (takes precedence over computed).
    """

    plt.rcParams.update({
    'font.size': 14,          
    'axes.titlesize': 16,     
    'axes.labelsize': 16,     
    'xtick.labelsize': 14,    
    'ytick.labelsize': 14,    
    'legend.fontsize': 14,    
    })

    max_layers = max(len(r) for r in rank_ratio_dict.values())
    blocks = list(rank_ratio_dict.keys())
    data = np.full((max_layers, len(blocks)), np.nan, dtype=float)
    for i, b in enumerate(blocks):
        for j, val in enumerate(rank_ratio_dict[b]):
            if j < max_layers:
                data[j, i] = val

    #Plot
    plt.figure(figsize=(12, 8))
    ax = sns.heatmap(
        data,
        annot=True, fmt=".2f",
        cmap="YlGnBu",
        xticklabels=blocks,
        yticklabels=np.arange(1, max_layers + 1),
        vmin=vmin, vmax=vmax,
        cbar_kws={'label': 'Rank Ratio'}
    )

    ax.invert_yaxis()

    plt.title(title, fontsize=14)
    plt.xlabel('Compressed Blocks', fontsize=12)
    plt.ylabel('Layer Index', fontsize=12)
    plt.tight_layout()
    plt.show()